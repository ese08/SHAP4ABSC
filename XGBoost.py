"""
This program trains the compare model based on XGBoost.
It trains the XGBoost model using iterative prediction results based on SHAP generated by the previous steps.

The program is mainly divided into three stages: data preparation, automatic parameter optimization using Optuna, and training the final XGBoost model with the best parameters.

The data preparation stage is to extract all iteration results from each sample and organize them into a feature vector.
The automatic parameter optimization stage is to use Optuna to optimize the hyperparameters of the XGBoost model.
The training stage is to train the XGBoost model with the best parameters.
"""
import json
import numpy as np
import pandas as pd
import optuna
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

"""
Prepare the data.
"""
json_path = Path("/Users/michaelz/Documents/Study/EUR/Third Year/Thesis/Codes/Code_save/new_trails/laptop_results.json")
with json_path.open("r", encoding="utf-8") as f:
    raw = json.load(f)

cfg = raw["experiment_info"]["thresholds"]
MSP_TH = cfg["msp"]
LABEL2ID = {"negative": 0, "neutral": 1, "positive": 2}
MAX_ITERS = 0
for item in raw["results"]:
    if len(item["iterations"]) > MAX_ITERS:
        MAX_ITERS = len(item["iterations"])

# extract all iteration results from each sample and organize into a feature vector
rows = []
for item in raw["results"]:
    guid = item["guid"]
    true_label = LABEL2ID[item["true_label"]]
    
    predictions = []
    msps = []
    margins = []
    entropies = []
    
    for it in item["iterations"]:
        predictions.append(LABEL2ID.get(it["prediction"], -1))
        msps.append(it["msp"])
        margins.append(it["predict_margin"])
        entropies.append(it["entropy"])
            

    num_iters = len(item["iterations"])
    unique_sentiments_count = len(set(predictions))
    first_iter = item["iterations"][0]
    early_success = (num_iters == 1)

    row = {
        "guid": guid,
        "label": true_label,
        "num_iterations": num_iters,
        "unique_sentiments_count": unique_sentiments_count,
        "early_success": early_success,
    }
    
    iter_features_map = {
        "pred": predictions,
        "msp": msps,
        "margin": margins,
        "entropy": entropies,
    }
    
    for feat_name, values in iter_features_map.items():
        for i in range(MAX_ITERS):
            value = values[i] if i < len(values) else 0.0
            row[f"{feat_name}_iter_{i+1}"] = value
            
    rows.append(row)

df = pd.DataFrame(rows)

feature_cols = [col for col in df.columns if col not in ["guid", "label", "early_success"]]
X_all = df[feature_cols].values
y_all = df["label"].values


# split the data into training and validation sets
X_train, X_val, y_train, y_val, df_train, df_val = train_test_split(
    X_all, y_all, df, test_size=0.2, random_state=42, stratify=y_all)

# only select samples that need to be iterated for training
train_iterated_mask = ~df_train["early_success"].values

X_train_final = X_train[train_iterated_mask]
y_train_final = y_train[train_iterated_mask]


""" 
Use Optuna to optimize hyperparameters. 
"""

def objective(trial):

    params = {
        "objective": "multi:softprob",
        "num_class": 3,
        "eval_metric": "mlogloss",
        "tree_method": "hist",
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'max_depth': trial.suggest_int('max_depth', 4, 10),
        'n_estimators': trial.suggest_int('n_estimators', 200, 800),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),
        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),
        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),
        "random_state": 42,
    }

    clf = XGBClassifier(**params, early_stopping_rounds=25)
    clf.fit(
        X_train_final, y_train_final,
        eval_set=[(X_val, y_val)],
        verbose=False
    )
    y_pred = np.argmax(clf.predict_proba(X_val), axis=1)
    accuracy = accuracy_score(y_val, y_pred)
    return accuracy

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50, show_progress_bar=True)
trial = study.best_trial


"""
Retrain the model with the best parameters.
"""

best_params = trial.params
clf = XGBClassifier(
    objective="multi:softprob",
    num_class=3,
    eval_metric="mlogloss",
    tree_method="hist",
    random_state=42,
    early_stopping_rounds=25,
    **best_params
)
clf.fit(
    X_train_final, y_train_final,
    eval_set=[(X_val, y_val)],
    verbose=False,
)

# get final predictions and calculate accuracy
val_early_success_mask = df_val["early_success"].values
xgb_preds_for_iterated = np.argmax(clf.predict_proba(X_val), axis=1)
first_iter_preds = df_val['pred_iter_1'].values
y_pred_final = np.where(val_early_success_mask, first_iter_preds, xgb_preds_for_iterated)
final_accuracy = accuracy_score(y_val, y_pred_final)
print(f"Final prediction accuracy: {final_accuracy:.4f}")

# save the model
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_filename = f"xgb_laptop_best_{timestamp}.json"
clf.save_model(model_filename)
print(f"Model saved to {model_filename}")
